{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Front Page Welcome to my research articles. For quite while, I feel it is extremely hard to get into a research field. One problem is that a research paper mostly focuses on the difference between its 10% contribution and the rest 90% of existing work, while the whole background knowledge or picture is missing. Many people believe the majority of research papers is useless. I think it's one indicator that some people may miss the whole picture and focus on some useless but doable directions. Unfortunately, when the whole picture is presented in a book or a survey, it is probably not a fresh research field anymore. As a trial, I feel it is worth to start tracking the thoughts running behind my research papers. No matter it is correct or not in the current stage(I may keep updating those), it may help me and other people to stay focusing on the whole picture and ensure a meaningful track of research.","title":"Home"},{"location":"#front-page","text":"Welcome to my research articles. For quite while, I feel it is extremely hard to get into a research field. One problem is that a research paper mostly focuses on the difference between its 10% contribution and the rest 90% of existing work, while the whole background knowledge or picture is missing. Many people believe the majority of research papers is useless. I think it's one indicator that some people may miss the whole picture and focus on some useless but doable directions. Unfortunately, when the whole picture is presented in a book or a survey, it is probably not a fresh research field anymore. As a trial, I feel it is worth to start tracking the thoughts running behind my research papers. No matter it is correct or not in the current stage(I may keep updating those), it may help me and other people to stay focusing on the whole picture and ensure a meaningful track of research.","title":"Front Page"},{"location":"absa/","text":"Aspect-based Sentiment Analysis Although a rating can summarize a whole review, it is really the vast amount of finer details matters a lot. After all, each person's need is quite different and we wish a personalized fit of a product (or service) to our own needs. Aspect-based sentiment analysis (ABSA) aims to find fine-grained opinions from reviews. For example, in a Laptop domain, we may wish to see whether the screen , keyboard , etc. are good or not. Although there are unlimited amount of reviews with coarse-grained ratings, ABSA severely lacks supervision from humans (e.g., in the form of annotated data). The above ABSA problem can be decomposed into two important sub-tasks: aspect extraction (AE) and aspect sentiment classification (ASC). Aspect Extraction Given a review sentence, such as The retina display is beautiful. , AE aims to find aspects retina display . In DL, it is mostly formalized as a sequence labeling problem: label \"The retina display is great .\" as \"O B I O O O\" so to extract \"retina display\" as an aspect. Obviously, the context of an aspect is important and an AI agent needs to have enough domain knowledge to support such extraction, such as A beautiful thing in Laptop could be an aspect. However, counting on the strong supervision from humans aspects-by-aspects for a particular domain is impossible. We show that a simple domain word embedding can boost the performance. More ideally, a language model can boost it further as whenever you see some aspects dropped, LM really encouraging the context words to recover that aspects. Aspect Sentiment Classification Given an aspect retina display and a review sentence The retina display is great. , ASC detects the polarity of that aspect positive . One challenge of ASC is to detect the polarity of opinion expressions and there could be unlimited amount of such expressions to annotate. Again language model can help this too as human tends to repeat their opinions in their writing so knowing one opinion may help to automatically understand the other. For example, in Terrible product. It could be the last thing I may consider to buy , we may infer the harder opinion of the second sentence from Terrible in the first one so to automatically learn unlimited expressions like that.","title":"Sentiment Analysis"},{"location":"absa/#aspect-based-sentiment-analysis","text":"Although a rating can summarize a whole review, it is really the vast amount of finer details matters a lot. After all, each person's need is quite different and we wish a personalized fit of a product (or service) to our own needs. Aspect-based sentiment analysis (ABSA) aims to find fine-grained opinions from reviews. For example, in a Laptop domain, we may wish to see whether the screen , keyboard , etc. are good or not. Although there are unlimited amount of reviews with coarse-grained ratings, ABSA severely lacks supervision from humans (e.g., in the form of annotated data). The above ABSA problem can be decomposed into two important sub-tasks: aspect extraction (AE) and aspect sentiment classification (ASC).","title":"Aspect-based Sentiment Analysis"},{"location":"absa/#aspect-extraction","text":"Given a review sentence, such as The retina display is beautiful. , AE aims to find aspects retina display . In DL, it is mostly formalized as a sequence labeling problem: label \"The retina display is great .\" as \"O B I O O O\" so to extract \"retina display\" as an aspect. Obviously, the context of an aspect is important and an AI agent needs to have enough domain knowledge to support such extraction, such as A beautiful thing in Laptop could be an aspect. However, counting on the strong supervision from humans aspects-by-aspects for a particular domain is impossible. We show that a simple domain word embedding can boost the performance. More ideally, a language model can boost it further as whenever you see some aspects dropped, LM really encouraging the context words to recover that aspects.","title":"Aspect Extraction"},{"location":"absa/#aspect-sentiment-classification","text":"Given an aspect retina display and a review sentence The retina display is great. , ASC detects the polarity of that aspect positive . One challenge of ASC is to detect the polarity of opinion expressions and there could be unlimited amount of such expressions to annotate. Again language model can help this too as human tends to repeat their opinions in their writing so knowing one opinion may help to automatically understand the other. For example, in Terrible product. It could be the last thing I may consider to buy , we may infer the harder opinion of the second sentence from Terrible in the first one so to automatically learn unlimited expressions like that.","title":"Aspect Sentiment Classification"},{"location":"overview/","text":"Data-intensive Generalization Deep learning (DL) has gained significant improvements over the past a few years, where back-propagation serves as the core driving force to learn features or representations from multiple parameterized layers automatically. This finally bridges the gap between the raw inputs (pixels for CV and sequence of chars for NLP) and the output of an ML task. As such, parameter-intensive DL models can consume much more data than traditional ML models to allow for data-intensive generalization. Looking forward, there is no free lunch for an ML model and DL is not perfect. As AI aims to free human from intensive labor work, we humans naturally apply constraints on an ML model. So neither intensive data annotation nor architecture design for a specific task is desirable. This rule out strongly supervised learning with tens of thousands of examples or architecture rich but parameter fewer models. In the end, we are looking for simple and general architectures with a huge amount of parameters to let unlabeled data to fill in. This leads to unsupervised (or self-supervised as there is no perfect unsupervised) representation learning, where training signals can be discovered from the input itself. Even though, the generalization from DL is still biased by the statistics of the intensive data. By statistics, we mean the majority wins in representation learning. But in real-world, the long-tail of many specifics determines the performance. What is even worse is that the trained agent is facing a dynamic world, where the old zero to minor data may need to dominate the representation later: task representation learning and open-world learning .","title":"Overview"},{"location":"overview/#data-intensive-generalization","text":"Deep learning (DL) has gained significant improvements over the past a few years, where back-propagation serves as the core driving force to learn features or representations from multiple parameterized layers automatically. This finally bridges the gap between the raw inputs (pixels for CV and sequence of chars for NLP) and the output of an ML task. As such, parameter-intensive DL models can consume much more data than traditional ML models to allow for data-intensive generalization. Looking forward, there is no free lunch for an ML model and DL is not perfect. As AI aims to free human from intensive labor work, we humans naturally apply constraints on an ML model. So neither intensive data annotation nor architecture design for a specific task is desirable. This rule out strongly supervised learning with tens of thousands of examples or architecture rich but parameter fewer models. In the end, we are looking for simple and general architectures with a huge amount of parameters to let unlabeled data to fill in. This leads to unsupervised (or self-supervised as there is no perfect unsupervised) representation learning, where training signals can be discovered from the input itself. Even though, the generalization from DL is still biased by the statistics of the intensive data. By statistics, we mean the majority wins in representation learning. But in real-world, the long-tail of many specifics determines the performance. What is even worse is that the trained agent is facing a dynamic world, where the old zero to minor data may need to dominate the representation later: task representation learning and open-world learning .","title":"Data-intensive Generalization"},{"location":"owl/","text":"Open-world Learning Background One goal of ML is to automatically discover the hidden casual process that explains the results of real-world, such as mimic and guesses the process of human thoughts so to make similar predictions. However, we humans live in a dynamic world and we keep adjusting our mental process to accommodate the evolving world. But in ML, the old distributions on training may not valid when the model is deployed. It doesn't make sense that in 99% cases a model is trained and deployed then never changed later. That is the major reason why people discussing generalization and a good testing score is always just a score but never means good performance in real-world. ML model should adjust their learned process on new distributions and the ability to change the behavior of a deployed model is a key point to be a general model. We roughly term this as an open-world learning problem. Training a model only as a Data Manipulator One observation from humans is that we do not overfit to many details of the input. Humans' generalization comes from the fact that learning is only on abstractive operation level and we perform good separation between data and their operations. For example, we may automatically forget many numbers but remember the math operations or details of many classes (like more than 10 classes) but as long as the data are presented, we can still perform good classification. Sadly, current ML, especially end2end deep learning, wish to learn everything from the data into the model. Inspired by the above discussion, we set our trial on taking a data manipulator as an approach to solving the open-world learning problem. We wish to see that when the data is changed during testing, the data manipulator (as an ML model) can still accommodate the change to a certain degree. Taking aspect extraction as an example, we show that a testing model can still change its performance as more testing data is available. Open-world Classification As a classic task, traditional classification task only focuses on closed-world learning, where the classes are pre-defined in the training data. The extreme case is when a new class comes during testing/prediction, how could that model handle that? Of course, it will assign one old class to an example belonging to a new class, which is obviously a mistake. The first step is we need to reject all new classes to make the results correct. Openset recognition is such a problem in CV and we also make a text-classification paper . This problem can be further decomposed as a combination of anomaly detection and closed-world classification and all unknown new classes should be detected first by the anomaly detection before passing into closed-world classification, which may not be a very novel problem. Going further, we still want to a classifier to support new classes (so to adapt to a new distribution), where you can always expand, for example, the dense layer before the softmax function with a new row of parameters. But this is still not perfect as new classes can still mixed-in with existing. Or say, we need the anomaly detection part to be open-world, too. So the rejection can also be dynamic. Open-world Classification As such, we really want the classifier to support unlimited classes and the set of known classes can be dynamically maintained, including both adding a new class or delete an old one. Any class not in the known set should be rejected. All the problem comes when the model remembered (overfit) a specific set of classes too much. We humans probably cache existing classes into our brain too much for a small set of classes. For a large set, as we cannot remember all classes, we actually do many comparisons: give an example, compare with existing examples in existing classes. If none of them is similar, we say we don't know those classes and take that as a new one. Now another example coming from that new class, if we find they are similar, we can do classification on that new class now. In the end, we probably only learn the comparison part but no specific class or specific set. Our paper aims to train such a comparator to manipulate an arbitrary set of classes.","title":"Open-world Learning"},{"location":"owl/#open-world-learning","text":"","title":"Open-world Learning"},{"location":"owl/#background","text":"One goal of ML is to automatically discover the hidden casual process that explains the results of real-world, such as mimic and guesses the process of human thoughts so to make similar predictions. However, we humans live in a dynamic world and we keep adjusting our mental process to accommodate the evolving world. But in ML, the old distributions on training may not valid when the model is deployed. It doesn't make sense that in 99% cases a model is trained and deployed then never changed later. That is the major reason why people discussing generalization and a good testing score is always just a score but never means good performance in real-world. ML model should adjust their learned process on new distributions and the ability to change the behavior of a deployed model is a key point to be a general model. We roughly term this as an open-world learning problem.","title":"Background"},{"location":"owl/#training-a-model-only-as-a-data-manipulator","text":"One observation from humans is that we do not overfit to many details of the input. Humans' generalization comes from the fact that learning is only on abstractive operation level and we perform good separation between data and their operations. For example, we may automatically forget many numbers but remember the math operations or details of many classes (like more than 10 classes) but as long as the data are presented, we can still perform good classification. Sadly, current ML, especially end2end deep learning, wish to learn everything from the data into the model. Inspired by the above discussion, we set our trial on taking a data manipulator as an approach to solving the open-world learning problem. We wish to see that when the data is changed during testing, the data manipulator (as an ML model) can still accommodate the change to a certain degree. Taking aspect extraction as an example, we show that a testing model can still change its performance as more testing data is available.","title":"Training a model only as a Data Manipulator"},{"location":"owl/#open-world-classification","text":"As a classic task, traditional classification task only focuses on closed-world learning, where the classes are pre-defined in the training data. The extreme case is when a new class comes during testing/prediction, how could that model handle that? Of course, it will assign one old class to an example belonging to a new class, which is obviously a mistake. The first step is we need to reject all new classes to make the results correct. Openset recognition is such a problem in CV and we also make a text-classification paper . This problem can be further decomposed as a combination of anomaly detection and closed-world classification and all unknown new classes should be detected first by the anomaly detection before passing into closed-world classification, which may not be a very novel problem. Going further, we still want to a classifier to support new classes (so to adapt to a new distribution), where you can always expand, for example, the dense layer before the softmax function with a new row of parameters. But this is still not perfect as new classes can still mixed-in with existing. Or say, we need the anomaly detection part to be open-world, too. So the rejection can also be dynamic. Open-world Classification As such, we really want the classifier to support unlimited classes and the set of known classes can be dynamically maintained, including both adding a new class or delete an old one. Any class not in the known set should be rejected. All the problem comes when the model remembered (overfit) a specific set of classes too much. We humans probably cache existing classes into our brain too much for a small set of classes. For a large set, as we cannot remember all classes, we actually do many comparisons: give an example, compare with existing examples in existing classes. If none of them is similar, we say we don't know those classes and take that as a new one. Now another example coming from that new class, if we find they are similar, we can do classification on that new class now. In the end, we probably only learn the comparison part but no specific class or specific set. Our paper aims to train such a comparator to manipulate an arbitrary set of classes.","title":"Open-world Classification"},{"location":"pub/","text":"Publication 2019 Open-world Learning and Application to Product Classification The Web Conference (WWW 2019) Hu Xu , Bing Liu, Lei Shu, P. Yu [ arxiv ], [ code ] BERT Post-Training for Review Reading Comprehension and Aspect-based Sentiment Analysis (using BERT for review-based tasks) 2019 Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL 2019) Hu Xu , Bing Liu, Lei Shu, Philip S. Yu [ arxiv ], [ code ], [dataset] 2018 Double Embeddings and CNN-based Sequence Labeling for Aspect Extraction the 56th Annual Meeting of the Association for Computational Linguistics (ACL 2018) (This paper won Yelp Dataset Challenge Round 12 Grand Prize Award) Hu Xu , Bing Liu, Lei Shu, Philip S. Yu [ paper ], [ code ], [ domain embedding ], [ bib ], [ poster ] Lifelong Domain Word Embedding via Meta-Learning International Joint Conference on Artificial Intelligence (IJCAI 2018) Hu Xu , Bing Liu, Lei Shu, Philip S. Yu [ arxiv ], [ code ], [ bib ], [ slides ] Dual Attention Network for Product Compatibility and Function Satisfiability Analysis AAAI Conference on Artificial Intelligence (AAAI 2018) (This paper focuses on complementary aspect extraction and polarity classification from question-answering pairs) Hu Xu , Sihong Xie, Lei Shu, Philip S. Yu [ paper ], [ dataset ], [ bib ], [ slides ] Incorporating the Structure of the Belief State in End-to-End Task-Oriented Dialogue Systems NeurIPS 2018 Conversational AI Workshop Lei Shu, Piero Molino, Mahdi Namazifar, Bing Liu, Hu Xu , Huaixiu Zheng, Gokhan Tur [ paper ] Unseen Class Discovery in Open-world Classification preprint arXiv:1801.05609 Lei Shu, Hu Xu , Bing Liu [ arxiv ] 2017 Product Function Need Recognition via Semi-supervised Attention Network IEEE International Conference on Big Data 2017 (IEEE Bigdata 2017) Hu Xu , Sihong Xie, Lei Shu, Philip S. Yu [ paper ], [ dataset ], [ bib ] DOC: Deep Open Classification of Text Documents 2017 Conference on Empirical Methods in Natural Language Processing (EMNLP 2017) Lei Shu, Hu Xu , Bing Liu [ paper ], [ bib ] Lifelong Learning CRF for Supervised Aspect Extraction the 55th Annual Meeting of the Association for Computational Linguistics (ACL 2017) Lei Shu, Hu Xu , Bing Liu [ paper ], [ bib ] 2016 Mining Compatible/Incompatible Entities from Question and Answering via Yes/No Answer Classification using Distant Label Expansion arXiv preprint arXiv:1612.04499 Hu Xu , Lei Shu, Jingyuan Zhang, Philip S. Yu [ paper ], [ dataset ], [ bib ] CER: Complementary Entity Recognition via Knowledge Expansion on Large Unlabeled Product Reviews (Previous title: Sentence-level Extraction of Complementary Entities using Large Unlabeled Product Reviews) IEEE International Conference on Big Data 2016 (IEEE Bigdata 2016) Hu Xu , Sihong Xie, Lei Shu, Philip S. Yu [ paper ], [ slides ], [ data ], [ bib ] Lifelong-RL: Lifelong Relaxation Labeling for Separating Entities and Aspects in Opinion Targets (Previous title: Separating entities and aspects in opinion targets using lifelong graph labeling) 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP 2016) Lei Shu, Bing Liu, Hu Xu , and Annice Kim [ paper ], [ slides ], [ bib ] 2013 Planning Paths with Fewer Turns on Grid Maps AAAI Sixth Annual Symposium on Combinatorial Search Hu Xu , Lei Shu, May Huang [ paper ], [ dataset ], [ bib ] High-speed and accurate laser scan matching using classified features IEEE International Symposium on Robotic and Sensors Environments (ROSE), 2013 Lei Shu, Hu Xu , May Huang [check IEEE database], [ bib ] 2011 Accuracy analysis of power characterization and modeling Convergence and Hybrid Information Technology Springer Berlin Heidelberg Xiaolan Bai, Hu Xu and May Huang [check Springer Database]","title":"Publication"},{"location":"pub/#publication","text":"","title":"Publication"},{"location":"pub/#2019","text":"Open-world Learning and Application to Product Classification The Web Conference (WWW 2019) Hu Xu , Bing Liu, Lei Shu, P. Yu [ arxiv ], [ code ] BERT Post-Training for Review Reading Comprehension and Aspect-based Sentiment Analysis (using BERT for review-based tasks) 2019 Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL 2019) Hu Xu , Bing Liu, Lei Shu, Philip S. Yu [ arxiv ], [ code ], [dataset]","title":"2019"},{"location":"pub/#2018","text":"Double Embeddings and CNN-based Sequence Labeling for Aspect Extraction the 56th Annual Meeting of the Association for Computational Linguistics (ACL 2018) (This paper won Yelp Dataset Challenge Round 12 Grand Prize Award) Hu Xu , Bing Liu, Lei Shu, Philip S. Yu [ paper ], [ code ], [ domain embedding ], [ bib ], [ poster ] Lifelong Domain Word Embedding via Meta-Learning International Joint Conference on Artificial Intelligence (IJCAI 2018) Hu Xu , Bing Liu, Lei Shu, Philip S. Yu [ arxiv ], [ code ], [ bib ], [ slides ] Dual Attention Network for Product Compatibility and Function Satisfiability Analysis AAAI Conference on Artificial Intelligence (AAAI 2018) (This paper focuses on complementary aspect extraction and polarity classification from question-answering pairs) Hu Xu , Sihong Xie, Lei Shu, Philip S. Yu [ paper ], [ dataset ], [ bib ], [ slides ] Incorporating the Structure of the Belief State in End-to-End Task-Oriented Dialogue Systems NeurIPS 2018 Conversational AI Workshop Lei Shu, Piero Molino, Mahdi Namazifar, Bing Liu, Hu Xu , Huaixiu Zheng, Gokhan Tur [ paper ] Unseen Class Discovery in Open-world Classification preprint arXiv:1801.05609 Lei Shu, Hu Xu , Bing Liu [ arxiv ]","title":"2018"},{"location":"pub/#2017","text":"Product Function Need Recognition via Semi-supervised Attention Network IEEE International Conference on Big Data 2017 (IEEE Bigdata 2017) Hu Xu , Sihong Xie, Lei Shu, Philip S. Yu [ paper ], [ dataset ], [ bib ] DOC: Deep Open Classification of Text Documents 2017 Conference on Empirical Methods in Natural Language Processing (EMNLP 2017) Lei Shu, Hu Xu , Bing Liu [ paper ], [ bib ] Lifelong Learning CRF for Supervised Aspect Extraction the 55th Annual Meeting of the Association for Computational Linguistics (ACL 2017) Lei Shu, Hu Xu , Bing Liu [ paper ], [ bib ]","title":"2017"},{"location":"pub/#2016","text":"Mining Compatible/Incompatible Entities from Question and Answering via Yes/No Answer Classification using Distant Label Expansion arXiv preprint arXiv:1612.04499 Hu Xu , Lei Shu, Jingyuan Zhang, Philip S. Yu [ paper ], [ dataset ], [ bib ] CER: Complementary Entity Recognition via Knowledge Expansion on Large Unlabeled Product Reviews (Previous title: Sentence-level Extraction of Complementary Entities using Large Unlabeled Product Reviews) IEEE International Conference on Big Data 2016 (IEEE Bigdata 2016) Hu Xu , Sihong Xie, Lei Shu, Philip S. Yu [ paper ], [ slides ], [ data ], [ bib ] Lifelong-RL: Lifelong Relaxation Labeling for Separating Entities and Aspects in Opinion Targets (Previous title: Separating entities and aspects in opinion targets using lifelong graph labeling) 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP 2016) Lei Shu, Bing Liu, Hu Xu , and Annice Kim [ paper ], [ slides ], [ bib ]","title":"2016"},{"location":"pub/#2013","text":"Planning Paths with Fewer Turns on Grid Maps AAAI Sixth Annual Symposium on Combinatorial Search Hu Xu , Lei Shu, May Huang [ paper ], [ dataset ], [ bib ] High-speed and accurate laser scan matching using classified features IEEE International Symposium on Robotic and Sensors Environments (ROSE), 2013 Lei Shu, Hu Xu , May Huang [check IEEE database], [ bib ]","title":"2013"},{"location":"pub/#2011","text":"Accuracy analysis of power characterization and modeling Convergence and Hybrid Information Technology Springer Berlin Heidelberg Xiaolan Bai, Hu Xu and May Huang [check Springer Database]","title":"2011"},{"location":"qa/","text":"Question Answering Review Reading Comprehension (RRC) Complementary Entity Recognition (CER)","title":"Question Answering"},{"location":"qa/#question-answering","text":"","title":"Question Answering"},{"location":"qa/#review-reading-comprehension-rrc","text":"","title":"Review Reading Comprehension (RRC)"},{"location":"qa/#complementary-entity-recognition-cer","text":"","title":"Complementary Entity Recognition (CER)"},{"location":"tdrl/","text":"Task and Domain Representation Learning Our research focus is on unsupervised (or self-supervised) domain representation learning as strong supervision from humans is not AI enough. Even DL has the data-intensive generalization, it still (and may always) suffers from the domain problem in two perspectives: (1) the distribution of data is changed in an end task and certain types of data never appear before; (2) the majority wins the representation where domain-specific representations are squeezed but later challenged by a domain task. For each perspective, we focus on two-levels of domain representation, driven by two major breakthroughs in NLP research on deep learning(word embedding and contextualized representation learning). Out-of-distribution Domain Representation Learning Although most NLP tasks are defined on formal writings such as articles from Wikipedia, informal texts are largely ignored in many NLP tasks but are in huge amount. As such, representations learned from formal texts are challenged by domains with informal texts (which may differ in style-of-writing or rich opinions). Consequently, the representation is not general enough. Word Level On the word level, we show that such change of domains makes word embeddings trained from Wikipedia not suitable for reviews. Context Level Similar things happen to as in contextualized representation learning. Via ablation study, we show that a language model ( BERT ) pre-trained on Wikipedia and Bookcorpus is incapable handle 3 review-based tasks well. Domain Representation Learning for Minor Data Domain issues also come in even you have the domain data presented in your training corpus but are just not in the majority. This is caused by either the minor domain is challenged by the end task or fixed size of representation learning such that minor data must make rooms for major data. For example, on the word-level, some well-known word embeddings are trained from web pages in the world, where a certain amount of reviews may present. But they are still a small fraction among all web pages and 300 dimension embeddings may not have enough room to save those minor details. We demonstrated that even GloVe is not perfect on review-based tasks. What is even interesting is that the domain problem is word-by-word , not domain-by-domain. For some general stop words, we believe the representation from the natural distribution of web pages win as those words should be used for any domain and GloVe provides such generality that a domain corpus may not give. For domain specific-words, we don't wish distributions from irrelevant domains bias them. For example, we may only wish bright is closely associated with screen or keyboard but not sun in a Laptop domain. Thus, the ideal case is to combine them together and we show a simple way to do that. Task Representation Learning In a more general perspective, the domain is just one aspect of an end task. Since we focus on unsupervised learning to avoid heavy human efforts on annotation for a specific task, this may lead to representations more general to a wide spectrum of tasks but less specific to an end task. This is more important in contextualized representation learning as to whether certain contexts are important or not is very task-specific. We show that less supervised training data for an end task is unable to learn enough task representation and certain methods to bridge the gap is needed.","title":"Task Representation Learning"},{"location":"tdrl/#task-and-domain-representation-learning","text":"Our research focus is on unsupervised (or self-supervised) domain representation learning as strong supervision from humans is not AI enough. Even DL has the data-intensive generalization, it still (and may always) suffers from the domain problem in two perspectives: (1) the distribution of data is changed in an end task and certain types of data never appear before; (2) the majority wins the representation where domain-specific representations are squeezed but later challenged by a domain task. For each perspective, we focus on two-levels of domain representation, driven by two major breakthroughs in NLP research on deep learning(word embedding and contextualized representation learning).","title":"Task and Domain Representation Learning"},{"location":"tdrl/#out-of-distribution-domain-representation-learning","text":"Although most NLP tasks are defined on formal writings such as articles from Wikipedia, informal texts are largely ignored in many NLP tasks but are in huge amount. As such, representations learned from formal texts are challenged by domains with informal texts (which may differ in style-of-writing or rich opinions). Consequently, the representation is not general enough.","title":"Out-of-distribution Domain Representation Learning"},{"location":"tdrl/#word-level","text":"On the word level, we show that such change of domains makes word embeddings trained from Wikipedia not suitable for reviews.","title":"Word Level"},{"location":"tdrl/#context-level","text":"Similar things happen to as in contextualized representation learning. Via ablation study, we show that a language model ( BERT ) pre-trained on Wikipedia and Bookcorpus is incapable handle 3 review-based tasks well.","title":"Context Level"},{"location":"tdrl/#domain-representation-learning-for-minor-data","text":"Domain issues also come in even you have the domain data presented in your training corpus but are just not in the majority. This is caused by either the minor domain is challenged by the end task or fixed size of representation learning such that minor data must make rooms for major data. For example, on the word-level, some well-known word embeddings are trained from web pages in the world, where a certain amount of reviews may present. But they are still a small fraction among all web pages and 300 dimension embeddings may not have enough room to save those minor details. We demonstrated that even GloVe is not perfect on review-based tasks. What is even interesting is that the domain problem is word-by-word , not domain-by-domain. For some general stop words, we believe the representation from the natural distribution of web pages win as those words should be used for any domain and GloVe provides such generality that a domain corpus may not give. For domain specific-words, we don't wish distributions from irrelevant domains bias them. For example, we may only wish bright is closely associated with screen or keyboard but not sun in a Laptop domain. Thus, the ideal case is to combine them together and we show a simple way to do that.","title":"Domain Representation Learning for Minor Data"},{"location":"tdrl/#task-representation-learning","text":"In a more general perspective, the domain is just one aspect of an end task. Since we focus on unsupervised learning to avoid heavy human efforts on annotation for a specific task, this may lead to representations more general to a wide spectrum of tasks but less specific to an end task. This is more important in contextualized representation learning as to whether certain contexts are important or not is very task-specific. We show that less supervised training data for an end task is unable to learn enough task representation and certain methods to bridge the gap is needed.","title":"Task Representation Learning"}]}