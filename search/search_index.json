{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Front Page Welcome to my research articles. For quite while, I feel it is extremely hard to get into a research field. One problem is that a research paper mostly focuses on the difference between its 10% contribution and the rest 90% of existing work, while the whole background knowledge or picture is missing. Many people believe the majority of research papers is useless. I think it's one indicator that some people may miss the whole picture and focus on some useless but doable directions. Unfortunately, when the whole picture is presented in a book or a survey, it is probably not a fresh research field anymore. As a trial, I feel it is worth to start tracking the thoughts running behind my research papers. No matter it is correct or not in the current stage(I may keep updating those), it may help me and other people to stay focusing on the whole picture and ensure a meaningful track of research.","title":"Home"},{"location":"#front-page","text":"Welcome to my research articles. For quite while, I feel it is extremely hard to get into a research field. One problem is that a research paper mostly focuses on the difference between its 10% contribution and the rest 90% of existing work, while the whole background knowledge or picture is missing. Many people believe the majority of research papers is useless. I think it's one indicator that some people may miss the whole picture and focus on some useless but doable directions. Unfortunately, when the whole picture is presented in a book or a survey, it is probably not a fresh research field anymore. As a trial, I feel it is worth to start tracking the thoughts running behind my research papers. No matter it is correct or not in the current stage(I may keep updating those), it may help me and other people to stay focusing on the whole picture and ensure a meaningful track of research.","title":"Front Page"},{"location":"absa/","text":"Aspect-based Sentiment Analysis Although a rating can summarize a whole review, it is really the vast amount of finer details matters a lot. After all, each person's need is quite different and we wish a personalized fit of a product (or service) to our own needs. Aspect-based sentiment analysis (ABSA) aims to find fine-grained opinions from reviews. For example, in a Laptop domain, we may wish to see whether the screen , keyboard , etc. are good or not. Although there are unlimited amount of reviews with coarse-grained ratings, ABSA severely lacks supervision from humans (e.g., in the form of annotated data). The above ABSA problem can be decomposed into two important sub-tasks: aspect extraction (AE) and aspect sentiment classification (ASC). Aspect Extraction Given a review sentence, such as The retina display is beautiful. , AE aims to find aspects retina display . In DL, it is mostly formalized as a sequence labeling problem: label \"The retina display is great .\" as \"O B I O O O\" so to extract \"retina display\" as an aspect. Obviously, the context of an aspect is important and an AI agent needs to have enough domain knowledge to support such extraction, such as A beautiful thing in Laptop could be an aspect. However, counting on the strong supervision from humans aspects-by-aspects for a particular domain is impossible. We show that a simple domain word embedding can boost the performance. More ideally, a language model can boost it further as whenever you see some aspects dropped, LM really encouraging the context words to recover that aspects. Aspect Sentiment Classification Given an aspect retina display and a review sentence The retina display is great. , ASC detects the polarity of that aspect positive . One challenge of ASC is to detect the polarity of opinion expressions and there could be unlimited amount of such expressions to annotate. Again language model can help this too as human tends to repeat their opinions in their writing so knowing one opinion may help to automatically understand the other. For example, in Terrible product. It could be the last thing I may consider to buy , we may infer the harder opinion of the second sentence from Terrible in the first one so to automatically learn unlimited expressions like that.","title":"Sentiment Analysis"},{"location":"absa/#aspect-based-sentiment-analysis","text":"Although a rating can summarize a whole review, it is really the vast amount of finer details matters a lot. After all, each person's need is quite different and we wish a personalized fit of a product (or service) to our own needs. Aspect-based sentiment analysis (ABSA) aims to find fine-grained opinions from reviews. For example, in a Laptop domain, we may wish to see whether the screen , keyboard , etc. are good or not. Although there are unlimited amount of reviews with coarse-grained ratings, ABSA severely lacks supervision from humans (e.g., in the form of annotated data). The above ABSA problem can be decomposed into two important sub-tasks: aspect extraction (AE) and aspect sentiment classification (ASC).","title":"Aspect-based Sentiment Analysis"},{"location":"absa/#aspect-extraction","text":"Given a review sentence, such as The retina display is beautiful. , AE aims to find aspects retina display . In DL, it is mostly formalized as a sequence labeling problem: label \"The retina display is great .\" as \"O B I O O O\" so to extract \"retina display\" as an aspect. Obviously, the context of an aspect is important and an AI agent needs to have enough domain knowledge to support such extraction, such as A beautiful thing in Laptop could be an aspect. However, counting on the strong supervision from humans aspects-by-aspects for a particular domain is impossible. We show that a simple domain word embedding can boost the performance. More ideally, a language model can boost it further as whenever you see some aspects dropped, LM really encouraging the context words to recover that aspects.","title":"Aspect Extraction"},{"location":"absa/#aspect-sentiment-classification","text":"Given an aspect retina display and a review sentence The retina display is great. , ASC detects the polarity of that aspect positive . One challenge of ASC is to detect the polarity of opinion expressions and there could be unlimited amount of such expressions to annotate. Again language model can help this too as human tends to repeat their opinions in their writing so knowing one opinion may help to automatically understand the other. For example, in Terrible product. It could be the last thing I may consider to buy , we may infer the harder opinion of the second sentence from Terrible in the first one so to automatically learn unlimited expressions like that.","title":"Aspect Sentiment Classification"},{"location":"hw/","text":"Build a Deep Learning Machine under $1000 As deep learning (DL) gradually domainates the research and job markets in AI, making a deep learning machine is essential for research and practice on getting up-to-date technology. The key difference between building a PC (including a gaming PC) for programming and AI is about choosing the right GPU for DL and its corresponding components. Although there are many good articles about choosing a good GPU with quantitive analysis, or building high-end hardware system (e.g., this or this ), I feel that discovering the most cost-effective but expandable strategy of building an entry-level system could be good for the majority of researchers or AI hobbyist (that are as frugal as me). Afterall, it doesn't make sense to spend a lot of money at the begining on high-end components and quickly the price dropped a lot but you didn't have enough time to learn from the old one. In general, the most cost-effective products in the world are usually sold in large volume. So in general, you may waste a lot money on making everything on high-end level with doubled or tripled prices to compensate their low volumes of sales. Note: this article is not intended to provide an up-to-date suggestion of configuration but a general idea for a long-term and cost-effective hardware guide for entry-level AI experiment. GPU GPU could be the only part that needs a high-end one as a DL machine is a GPU-centered system. You may check this article for the GPU you want, or check how much you left after deciding the rest. We assume a single GPU setting but other components are supposed to work for 2~3 GPUs so you may add more when newer ones come out. A few important facts about GPU: First, a newer GPU may typically have double or more performance in recent years (like the boost of 10 series from 9 or 20 series with tensor cores that supports fp16, which may speed up 3~4x and reduce the consumed memory by half). So I feel it doesn't make much sense to go for multiple GPUs at the beginning just for performance. As for research or simple practice, a multi-GPU setting may introduce extra coding and make your code impossible for many other researchers to repeat if they don't have multiple ones. And maybe just after one year, a newer single GPU is always better than your 2~3 old ones with no extra coding and no slow inter-GPU communications. Second, the size of GPU memory matters a lot, where some big models may not able to run for a GPU with a small memory. This could be a big problem as it is not a performance issue that you can pay more time to run but could be totally not working (e.g., using a small batch to save memory but the training is not stable). We will discuss the rest components first so all the budget leftover can be used for your GPU, as discussed in the summary. CPU As the computation load is mostly on the GPU, a cost-effective CPU that can handle a few processes is good enough. Note that most high-end CPUs are designed for doubled or tripled cores/threads, not much for frequency (or speed) on a single process. So an ideal CPU should be on the low-end like almost free so later you can replace it with no regret. My first CPU is just a $50 pentium for a single CPU. But I suggest to use i3 (around $120) to support 2~3 GPUs (I didn't see much difference when compare them with an i5). Plus, you may get a free fan from a low-end CPU. Motherboard A good motherboard is essential for expandability. So make sure the combination of CPU/motherboard and power supply can match each other within 1~2 years of upgrades. As we aim for 2~3 GPUs in future, make sure you have 2~3 PCIe slots. PCIe should be configured at 2 * 8x for two GPUs or 2 * 8x + 4x for 3 GPUs. In most cases, one GPU runs on 8x won't hurt your performance much (compared one one 16x) but allow you to install two GPUs. PCIe 4x may throttle the performance but its OK for my NLP applications. Of course, there's no problem when you just have one GPU running at 16x and almost all motherboard can support that. I assume we may upgrade to 64GB memory in future (16GB per module), so we possibly need a motherboard with 4 memory slots. Open-box/refurbished motherboard may save a lot but still have good enough quality for 1~2 years. I have an open-boxed ASUS prime for 2 years at $80. It still works quite well now. It also comes with a (debug) power button (so I have the chance to not buy a case at all). Note that some sellers like Microcenter may have a combo with CPU to save $30, even including their open-boxed motherboard. Memory The good news is that the crazily high memory price is gone. Now you can easily pick a new 16GB memory for $75, which is usually good for 1~2 GPUs (except some poorly writen python project). SSD For the first storage device, I think no need to consider a hard disk but just go for SSD because many DL models are large and slow to load/save checkpoints. Make sure it's at least $500 GB (about $60) and 1TB could be better. Also, check if you have some external hard disks so to store not-in-use DL projects out there in future. Power Supply Consider to have a power supply to support 2~3 GPUs ahead (250w for a 1080Ti, 260w for a 2080Ti). These add up to at least 900w for 3 GPUs. I have a 850w open-boxed Corsair one with warranty for $80. Accessories To be frugal, I won't discuss things like keyboard or mouse and I assume you have at least a normally working one for those and a laptop. So after installing the OS and basic configuration. You can use your laptop to remotely run everything. Also you may consider a $30 case but I totally run my system in the air and my motherboard has a power button so no need to short-circuit the pins to power up and have a configuration that is easy to upgrade. Summary As a summary, we have a $120 CPU + $80 motherboard + $75 memory + $60 SSD + $80 power supply = $415. If I assume my budget is $1000, then I have $585 for GPU, which is good enough for a 2070 (as recommended by many articles) and close to a 2080. It depends on your budget to try 2080 Ti/Titan RTX.","title":"Build a Deep Learning Machine under $1000"},{"location":"hw/#build-a-deep-learning-machine-under-1000","text":"As deep learning (DL) gradually domainates the research and job markets in AI, making a deep learning machine is essential for research and practice on getting up-to-date technology. The key difference between building a PC (including a gaming PC) for programming and AI is about choosing the right GPU for DL and its corresponding components. Although there are many good articles about choosing a good GPU with quantitive analysis, or building high-end hardware system (e.g., this or this ), I feel that discovering the most cost-effective but expandable strategy of building an entry-level system could be good for the majority of researchers or AI hobbyist (that are as frugal as me). Afterall, it doesn't make sense to spend a lot of money at the begining on high-end components and quickly the price dropped a lot but you didn't have enough time to learn from the old one. In general, the most cost-effective products in the world are usually sold in large volume. So in general, you may waste a lot money on making everything on high-end level with doubled or tripled prices to compensate their low volumes of sales. Note: this article is not intended to provide an up-to-date suggestion of configuration but a general idea for a long-term and cost-effective hardware guide for entry-level AI experiment.","title":"Build a Deep Learning Machine under $1000"},{"location":"hw/#gpu","text":"GPU could be the only part that needs a high-end one as a DL machine is a GPU-centered system. You may check this article for the GPU you want, or check how much you left after deciding the rest. We assume a single GPU setting but other components are supposed to work for 2~3 GPUs so you may add more when newer ones come out. A few important facts about GPU: First, a newer GPU may typically have double or more performance in recent years (like the boost of 10 series from 9 or 20 series with tensor cores that supports fp16, which may speed up 3~4x and reduce the consumed memory by half). So I feel it doesn't make much sense to go for multiple GPUs at the beginning just for performance. As for research or simple practice, a multi-GPU setting may introduce extra coding and make your code impossible for many other researchers to repeat if they don't have multiple ones. And maybe just after one year, a newer single GPU is always better than your 2~3 old ones with no extra coding and no slow inter-GPU communications. Second, the size of GPU memory matters a lot, where some big models may not able to run for a GPU with a small memory. This could be a big problem as it is not a performance issue that you can pay more time to run but could be totally not working (e.g., using a small batch to save memory but the training is not stable). We will discuss the rest components first so all the budget leftover can be used for your GPU, as discussed in the summary.","title":"GPU"},{"location":"hw/#cpu","text":"As the computation load is mostly on the GPU, a cost-effective CPU that can handle a few processes is good enough. Note that most high-end CPUs are designed for doubled or tripled cores/threads, not much for frequency (or speed) on a single process. So an ideal CPU should be on the low-end like almost free so later you can replace it with no regret. My first CPU is just a $50 pentium for a single CPU. But I suggest to use i3 (around $120) to support 2~3 GPUs (I didn't see much difference when compare them with an i5). Plus, you may get a free fan from a low-end CPU.","title":"CPU"},{"location":"hw/#motherboard","text":"A good motherboard is essential for expandability. So make sure the combination of CPU/motherboard and power supply can match each other within 1~2 years of upgrades. As we aim for 2~3 GPUs in future, make sure you have 2~3 PCIe slots. PCIe should be configured at 2 * 8x for two GPUs or 2 * 8x + 4x for 3 GPUs. In most cases, one GPU runs on 8x won't hurt your performance much (compared one one 16x) but allow you to install two GPUs. PCIe 4x may throttle the performance but its OK for my NLP applications. Of course, there's no problem when you just have one GPU running at 16x and almost all motherboard can support that. I assume we may upgrade to 64GB memory in future (16GB per module), so we possibly need a motherboard with 4 memory slots. Open-box/refurbished motherboard may save a lot but still have good enough quality for 1~2 years. I have an open-boxed ASUS prime for 2 years at $80. It still works quite well now. It also comes with a (debug) power button (so I have the chance to not buy a case at all). Note that some sellers like Microcenter may have a combo with CPU to save $30, even including their open-boxed motherboard.","title":"Motherboard"},{"location":"hw/#memory","text":"The good news is that the crazily high memory price is gone. Now you can easily pick a new 16GB memory for $75, which is usually good for 1~2 GPUs (except some poorly writen python project).","title":"Memory"},{"location":"hw/#ssd","text":"For the first storage device, I think no need to consider a hard disk but just go for SSD because many DL models are large and slow to load/save checkpoints. Make sure it's at least $500 GB (about $60) and 1TB could be better. Also, check if you have some external hard disks so to store not-in-use DL projects out there in future.","title":"SSD"},{"location":"hw/#power-supply","text":"Consider to have a power supply to support 2~3 GPUs ahead (250w for a 1080Ti, 260w for a 2080Ti). These add up to at least 900w for 3 GPUs. I have a 850w open-boxed Corsair one with warranty for $80.","title":"Power Supply"},{"location":"hw/#accessories","text":"To be frugal, I won't discuss things like keyboard or mouse and I assume you have at least a normally working one for those and a laptop. So after installing the OS and basic configuration. You can use your laptop to remotely run everything. Also you may consider a $30 case but I totally run my system in the air and my motherboard has a power button so no need to short-circuit the pins to power up and have a configuration that is easy to upgrade.","title":"Accessories"},{"location":"hw/#summary","text":"As a summary, we have a $120 CPU + $80 motherboard + $75 memory + $60 SSD + $80 power supply = $415. If I assume my budget is $1000, then I have $585 for GPU, which is good enough for a 2070 (as recommended by many articles) and close to a 2080. It depends on your budget to try 2080 Ti/Titan RTX.","title":"Summary"},{"location":"overview/","text":"Overview Deep learning (DL) has gained significant improvements over the past a few years, where back-propagation serves as the core driving force to learn features or representations from multiple parameterized layers automatically. This finally bridges the gap between the raw inputs (pixels for CV and sequence of chars for NLP) and the output of an ML task. As such, parameter-intensive DL models can consume much more data than traditional ML models to allow for data-intensive generalization and as a result, better performance. Learning with less human efforts Looking forward, there is no free lunch for an ML model and DL is not perfect. As AI aims to free human from intensive labor work, we humans naturally apply constraints(needs) on an ML model. So neither intensive data annotation nor architecture design for a specific task is desirable. This rule out strongly supervised learning with tens of thousands of examples or architecture rich but parameter fewer models. In the end, we are looking for simple and general architectures with a huge amount of parameters to let unlabeled data to fill in. This leads to unsupervised (or self-supervised as there is no perfect unsupervised) representation learning, where training signals can be discovered from the input itself. Statistical Generalization Even though, the generalization from DL is still biased by the statistics of the intensive data. By statistics, we mean the majority wins in representation learning. But in real-world, the long-tail of many specifics determines the performance. What is even worse is that the trained agent is facing a dynamic world, where some data seldomly or never appear before may need to dominate the distribution later. Two directions we focus on are task representation learning and open-world learning . In contrast, the i.i.d assumption from a ML model leads us a frequently mistake in research. We drop the timestamp TOO MUCH in testing and 99% of existing datasets lost temporal dependency of examples. By temporal dependency, I mean a USB 3.0 in training but a USB 2.0 in testing; or a iPhone Case in training but a iPhone in testing of recommender system. Randomly shuffling the available data (to manually make the testing has similar distribution as training) and spliting 20% for testing are totally wrong. This may explain why a model with good testing performance has poor performance when facing with human evaluation or real-world deployment. Obviously, a model trained from such a time-distorted dataset can let the model learn very easy post-hoc fact and not recover the hidden causal mechanism of data generation in real-world.","title":"Overview"},{"location":"overview/#overview","text":"Deep learning (DL) has gained significant improvements over the past a few years, where back-propagation serves as the core driving force to learn features or representations from multiple parameterized layers automatically. This finally bridges the gap between the raw inputs (pixels for CV and sequence of chars for NLP) and the output of an ML task. As such, parameter-intensive DL models can consume much more data than traditional ML models to allow for data-intensive generalization and as a result, better performance.","title":"Overview"},{"location":"overview/#learning-with-less-human-efforts","text":"Looking forward, there is no free lunch for an ML model and DL is not perfect. As AI aims to free human from intensive labor work, we humans naturally apply constraints(needs) on an ML model. So neither intensive data annotation nor architecture design for a specific task is desirable. This rule out strongly supervised learning with tens of thousands of examples or architecture rich but parameter fewer models. In the end, we are looking for simple and general architectures with a huge amount of parameters to let unlabeled data to fill in. This leads to unsupervised (or self-supervised as there is no perfect unsupervised) representation learning, where training signals can be discovered from the input itself.","title":"Learning with less human efforts"},{"location":"overview/#statistical-generalization","text":"Even though, the generalization from DL is still biased by the statistics of the intensive data. By statistics, we mean the majority wins in representation learning. But in real-world, the long-tail of many specifics determines the performance. What is even worse is that the trained agent is facing a dynamic world, where some data seldomly or never appear before may need to dominate the distribution later. Two directions we focus on are task representation learning and open-world learning . In contrast, the i.i.d assumption from a ML model leads us a frequently mistake in research. We drop the timestamp TOO MUCH in testing and 99% of existing datasets lost temporal dependency of examples. By temporal dependency, I mean a USB 3.0 in training but a USB 2.0 in testing; or a iPhone Case in training but a iPhone in testing of recommender system. Randomly shuffling the available data (to manually make the testing has similar distribution as training) and spliting 20% for testing are totally wrong. This may explain why a model with good testing performance has poor performance when facing with human evaluation or real-world deployment. Obviously, a model trained from such a time-distorted dataset can let the model learn very easy post-hoc fact and not recover the hidden causal mechanism of data generation in real-world.","title":"Statistical Generalization"},{"location":"owl/","text":"Open-world Learning Background One goal of ML is to automatically discover the hidden casual process that explains the results of real-world, such as mimic and guesses the process of human thoughts so to make similar predictions. However, we humans live in a dynamic world and we keep adjusting our mental process to accommodate the evolving world. But in ML, the old distributions on training may not valid when the model is deployed. It doesn't make sense that in 99% cases a model is trained and deployed then never changed later. That is the major reason why people discussing generalization and a good testing score is always just a score but never means good performance in real-world. ML model should adjust their learned process on new distributions and the ability to change the behavior of a deployed model is a key point to be a general model. We roughly term this as an open-world learning problem. Training a model only as a Data Manipulator One observation from humans is that we do not overfit to many details of the input. Humans' generalization comes from the fact that learning is only on abstractive operation level and we perform good separation between data and their operations. For example, we may automatically forget many numbers but remember the math operations or details of many classes (like more than 10 classes) but as long as the data are presented, we can still perform good classification. Sadly, current ML, especially end2end deep learning, wish to learn everything from the data into the model. Inspired by the above discussion, we set our trial on taking a data manipulator as an approach to solving the open-world learning problem. We wish to see that when the data is changed during testing, the data manipulator (as an ML model) can still accommodate the change to a certain degree. Taking aspect extraction as an example, we show that a testing model can still change its performance as more testing data is available. Open-world Classification As a classic task, traditional classification task only focuses on closed-world learning, where the classes are pre-defined in the training data. The extreme case is when a new class comes during testing/prediction, how could that model handle that? Of course, it will assign one old class to an example belonging to a new class, which is obviously a mistake. The first step is we need to reject all new classes to make the results correct. Openset recognition is such a problem in CV and we also make a text-classification paper . This problem can be further decomposed as a combination of anomaly detection and closed-world classification and all unknown new classes should be detected first by the anomaly detection before passing into closed-world classification, which may not be a very novel problem. Going further, we still want to a classifier to support new classes (so to adapt to a new distribution), where you can always expand, for example, the dense layer before the softmax function with a new row of parameters. But this is still not perfect as new classes can still mixed-in with existing. Or say, we need the anomaly detection part to be open-world, too. So the rejection can also be dynamic. Open-world Classification As such, we really want the classifier to support unlimited classes and the set of known classes can be dynamically maintained, including both adding a new class or delete an old one. Any class not in the known set should be rejected. All the problem comes when the model remembered (overfit) a specific set of classes too much. We humans probably cache existing classes into our brain too much for a small set of classes. For a large set, as we cannot remember all classes, we actually do many comparisons: give an example, compare with existing examples in existing classes. If none of them is similar, we say we don't know those classes and take that as a new one. Now another example coming from that new class, if we find they are similar, we can do classification on that new class now. In the end, we probably only learn the comparison part but no specific class or specific set. Our paper aims to train such a comparator to manipulate an arbitrary set of classes.","title":"Open-world Learning"},{"location":"owl/#open-world-learning","text":"","title":"Open-world Learning"},{"location":"owl/#background","text":"One goal of ML is to automatically discover the hidden casual process that explains the results of real-world, such as mimic and guesses the process of human thoughts so to make similar predictions. However, we humans live in a dynamic world and we keep adjusting our mental process to accommodate the evolving world. But in ML, the old distributions on training may not valid when the model is deployed. It doesn't make sense that in 99% cases a model is trained and deployed then never changed later. That is the major reason why people discussing generalization and a good testing score is always just a score but never means good performance in real-world. ML model should adjust their learned process on new distributions and the ability to change the behavior of a deployed model is a key point to be a general model. We roughly term this as an open-world learning problem.","title":"Background"},{"location":"owl/#training-a-model-only-as-a-data-manipulator","text":"One observation from humans is that we do not overfit to many details of the input. Humans' generalization comes from the fact that learning is only on abstractive operation level and we perform good separation between data and their operations. For example, we may automatically forget many numbers but remember the math operations or details of many classes (like more than 10 classes) but as long as the data are presented, we can still perform good classification. Sadly, current ML, especially end2end deep learning, wish to learn everything from the data into the model. Inspired by the above discussion, we set our trial on taking a data manipulator as an approach to solving the open-world learning problem. We wish to see that when the data is changed during testing, the data manipulator (as an ML model) can still accommodate the change to a certain degree. Taking aspect extraction as an example, we show that a testing model can still change its performance as more testing data is available.","title":"Training a model only as a Data Manipulator"},{"location":"owl/#open-world-classification","text":"As a classic task, traditional classification task only focuses on closed-world learning, where the classes are pre-defined in the training data. The extreme case is when a new class comes during testing/prediction, how could that model handle that? Of course, it will assign one old class to an example belonging to a new class, which is obviously a mistake. The first step is we need to reject all new classes to make the results correct. Openset recognition is such a problem in CV and we also make a text-classification paper . This problem can be further decomposed as a combination of anomaly detection and closed-world classification and all unknown new classes should be detected first by the anomaly detection before passing into closed-world classification, which may not be a very novel problem. Going further, we still want to a classifier to support new classes (so to adapt to a new distribution), where you can always expand, for example, the dense layer before the softmax function with a new row of parameters. But this is still not perfect as new classes can still mixed-in with existing. Or say, we need the anomaly detection part to be open-world, too. So the rejection can also be dynamic. Open-world Classification As such, we really want the classifier to support unlimited classes and the set of known classes can be dynamically maintained, including both adding a new class or delete an old one. Any class not in the known set should be rejected. All the problem comes when the model remembered (overfit) a specific set of classes too much. We humans probably cache existing classes into our brain too much for a small set of classes. For a large set, as we cannot remember all classes, we actually do many comparisons: give an example, compare with existing examples in existing classes. If none of them is similar, we say we don't know those classes and take that as a new one. Now another example coming from that new class, if we find they are similar, we can do classification on that new class now. In the end, we probably only learn the comparison part but no specific class or specific set. Our paper aims to train such a comparator to manipulate an arbitrary set of classes.","title":"Open-world Classification"},{"location":"pub/","text":"Publication 2019 Open-world Learning and Application to Product Classification The Web Conference (WWW 2019) Hu Xu , Bing Liu, Lei Shu, P. Yu [ arxiv ], [ code ] BERT Post-Training for Review Reading Comprehension and Aspect-based Sentiment Analysis (using BERT for review-based tasks) 2019 Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL 2019) Hu Xu , Bing Liu, Lei Shu, Philip S. Yu [ arxiv ], [ code ], [dataset] 2018 Double Embeddings and CNN-based Sequence Labeling for Aspect Extraction the 56th Annual Meeting of the Association for Computational Linguistics (ACL 2018) (This paper won Yelp Dataset Challenge Round 12 Grand Prize Award) Hu Xu , Bing Liu, Lei Shu, Philip S. Yu [ paper ], [ code ], [ domain embedding ], [ bib ], [ poster ] Lifelong Domain Word Embedding via Meta-Learning International Joint Conference on Artificial Intelligence (IJCAI 2018) Hu Xu , Bing Liu, Lei Shu, Philip S. Yu [ arxiv ], [ code ], [ bib ], [ slides ] Dual Attention Network for Product Compatibility and Function Satisfiability Analysis AAAI Conference on Artificial Intelligence (AAAI 2018) (This paper focuses on complementary aspect extraction and polarity classification from question-answering pairs) Hu Xu , Sihong Xie, Lei Shu, Philip S. Yu [ paper ], [ dataset ], [ bib ], [ slides ] Incorporating the Structure of the Belief State in End-to-End Task-Oriented Dialogue Systems NeurIPS 2018 Conversational AI Workshop Lei Shu, Piero Molino, Mahdi Namazifar, Bing Liu, Hu Xu , Huaixiu Zheng, Gokhan Tur [ paper ] Unseen Class Discovery in Open-world Classification preprint arXiv:1801.05609 Lei Shu, Hu Xu , Bing Liu [ arxiv ] 2017 Product Function Need Recognition via Semi-supervised Attention Network IEEE International Conference on Big Data 2017 (IEEE Bigdata 2017) Hu Xu , Sihong Xie, Lei Shu, Philip S. Yu [ paper ], [ dataset ], [ bib ] DOC: Deep Open Classification of Text Documents 2017 Conference on Empirical Methods in Natural Language Processing (EMNLP 2017) Lei Shu, Hu Xu , Bing Liu [ paper ], [ bib ] Lifelong Learning CRF for Supervised Aspect Extraction the 55th Annual Meeting of the Association for Computational Linguistics (ACL 2017) Lei Shu, Hu Xu , Bing Liu [ paper ], [ bib ] 2016 Mining Compatible/Incompatible Entities from Question and Answering via Yes/No Answer Classification using Distant Label Expansion arXiv preprint arXiv:1612.04499 Hu Xu , Lei Shu, Jingyuan Zhang, Philip S. Yu [ paper ], [ dataset ], [ bib ] CER: Complementary Entity Recognition via Knowledge Expansion on Large Unlabeled Product Reviews (Previous title: Sentence-level Extraction of Complementary Entities using Large Unlabeled Product Reviews) IEEE International Conference on Big Data 2016 (IEEE Bigdata 2016) Hu Xu , Sihong Xie, Lei Shu, Philip S. Yu [ paper ], [ slides ], [ data ], [ bib ] Lifelong-RL: Lifelong Relaxation Labeling for Separating Entities and Aspects in Opinion Targets (Previous title: Separating entities and aspects in opinion targets using lifelong graph labeling) 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP 2016) Lei Shu, Bing Liu, Hu Xu , and Annice Kim [ paper ], [ slides ], [ bib ] 2013 Planning Paths with Fewer Turns on Grid Maps AAAI Sixth Annual Symposium on Combinatorial Search Hu Xu , Lei Shu, May Huang [ paper ], [ dataset ], [ bib ] High-speed and accurate laser scan matching using classified features IEEE International Symposium on Robotic and Sensors Environments (ROSE), 2013 Lei Shu, Hu Xu , May Huang [check IEEE database], [ bib ] 2011 Accuracy analysis of power characterization and modeling Convergence and Hybrid Information Technology Springer Berlin Heidelberg Xiaolan Bai, Hu Xu and May Huang [check Springer Database]","title":"Publication"},{"location":"pub/#publication","text":"","title":"Publication"},{"location":"pub/#2019","text":"Open-world Learning and Application to Product Classification The Web Conference (WWW 2019) Hu Xu , Bing Liu, Lei Shu, P. Yu [ arxiv ], [ code ] BERT Post-Training for Review Reading Comprehension and Aspect-based Sentiment Analysis (using BERT for review-based tasks) 2019 Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL 2019) Hu Xu , Bing Liu, Lei Shu, Philip S. Yu [ arxiv ], [ code ], [dataset]","title":"2019"},{"location":"pub/#2018","text":"Double Embeddings and CNN-based Sequence Labeling for Aspect Extraction the 56th Annual Meeting of the Association for Computational Linguistics (ACL 2018) (This paper won Yelp Dataset Challenge Round 12 Grand Prize Award) Hu Xu , Bing Liu, Lei Shu, Philip S. Yu [ paper ], [ code ], [ domain embedding ], [ bib ], [ poster ] Lifelong Domain Word Embedding via Meta-Learning International Joint Conference on Artificial Intelligence (IJCAI 2018) Hu Xu , Bing Liu, Lei Shu, Philip S. Yu [ arxiv ], [ code ], [ bib ], [ slides ] Dual Attention Network for Product Compatibility and Function Satisfiability Analysis AAAI Conference on Artificial Intelligence (AAAI 2018) (This paper focuses on complementary aspect extraction and polarity classification from question-answering pairs) Hu Xu , Sihong Xie, Lei Shu, Philip S. Yu [ paper ], [ dataset ], [ bib ], [ slides ] Incorporating the Structure of the Belief State in End-to-End Task-Oriented Dialogue Systems NeurIPS 2018 Conversational AI Workshop Lei Shu, Piero Molino, Mahdi Namazifar, Bing Liu, Hu Xu , Huaixiu Zheng, Gokhan Tur [ paper ] Unseen Class Discovery in Open-world Classification preprint arXiv:1801.05609 Lei Shu, Hu Xu , Bing Liu [ arxiv ]","title":"2018"},{"location":"pub/#2017","text":"Product Function Need Recognition via Semi-supervised Attention Network IEEE International Conference on Big Data 2017 (IEEE Bigdata 2017) Hu Xu , Sihong Xie, Lei Shu, Philip S. Yu [ paper ], [ dataset ], [ bib ] DOC: Deep Open Classification of Text Documents 2017 Conference on Empirical Methods in Natural Language Processing (EMNLP 2017) Lei Shu, Hu Xu , Bing Liu [ paper ], [ bib ] Lifelong Learning CRF for Supervised Aspect Extraction the 55th Annual Meeting of the Association for Computational Linguistics (ACL 2017) Lei Shu, Hu Xu , Bing Liu [ paper ], [ bib ]","title":"2017"},{"location":"pub/#2016","text":"Mining Compatible/Incompatible Entities from Question and Answering via Yes/No Answer Classification using Distant Label Expansion arXiv preprint arXiv:1612.04499 Hu Xu , Lei Shu, Jingyuan Zhang, Philip S. Yu [ paper ], [ dataset ], [ bib ] CER: Complementary Entity Recognition via Knowledge Expansion on Large Unlabeled Product Reviews (Previous title: Sentence-level Extraction of Complementary Entities using Large Unlabeled Product Reviews) IEEE International Conference on Big Data 2016 (IEEE Bigdata 2016) Hu Xu , Sihong Xie, Lei Shu, Philip S. Yu [ paper ], [ slides ], [ data ], [ bib ] Lifelong-RL: Lifelong Relaxation Labeling for Separating Entities and Aspects in Opinion Targets (Previous title: Separating entities and aspects in opinion targets using lifelong graph labeling) 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP 2016) Lei Shu, Bing Liu, Hu Xu , and Annice Kim [ paper ], [ slides ], [ bib ]","title":"2016"},{"location":"pub/#2013","text":"Planning Paths with Fewer Turns on Grid Maps AAAI Sixth Annual Symposium on Combinatorial Search Hu Xu , Lei Shu, May Huang [ paper ], [ dataset ], [ bib ] High-speed and accurate laser scan matching using classified features IEEE International Symposium on Robotic and Sensors Environments (ROSE), 2013 Lei Shu, Hu Xu , May Huang [check IEEE database], [ bib ]","title":"2013"},{"location":"pub/#2011","text":"Accuracy analysis of power characterization and modeling Convergence and Hybrid Information Technology Springer Berlin Heidelberg Xiaolan Bai, Hu Xu and May Huang [check Springer Database]","title":"2011"},{"location":"qa/","text":"Question Answering Review Reading Comprehension (RRC) Complementary Entity Recognition (CER)","title":"Question Answering"},{"location":"qa/#question-answering","text":"","title":"Question Answering"},{"location":"qa/#review-reading-comprehension-rrc","text":"","title":"Review Reading Comprehension (RRC)"},{"location":"qa/#complementary-entity-recognition-cer","text":"","title":"Complementary Entity Recognition (CER)"},{"location":"tdrl/","text":"Task and Domain Representation Learning Our research focus is on unsupervised (or self-supervised) domain representation learning as strong supervision from humans is not AI enough. Even DL has the data-intensive generalization, it still (and may always) suffers from the domain problem in two perspectives: (1) the distribution of data is changed in an end task and certain types of data never appear before; (2) the majority wins the representation where domain-specific representations are squeezed but later challenged by a domain task. For each perspective, we focus on two-levels of domain representation, driven by two major breakthroughs in NLP research on deep learning(word embedding and contextualized representation learning). Out-of-distribution Domain Representation Learning Although most NLP tasks are defined on formal writings such as articles from Wikipedia, informal texts are largely ignored in many NLP tasks but are in huge amount. As such, representations learned from formal texts are challenged by domains with informal texts (which may differ in style-of-writing or rich opinions). Consequently, the representation is not general enough. Word Level On the word level, we show that such change of domains makes word embeddings trained from Wikipedia not suitable for reviews. Context Level Similar things happen to as in contextualized representation learning. Via ablation study, we show that a language model ( BERT ) pre-trained on Wikipedia and Bookcorpus is incapable handle 3 review-based tasks well. Domain Representation Learning for Minor Data Domain issues also come in even you have the domain data presented in your training corpus but are just not in the majority. This is caused by either the minor domain is challenged by the end task or fixed size of representation learning such that minor data must make rooms for major data. For example, on the word-level, some well-known word embeddings are trained from web pages in the world, where a certain amount of reviews may present. But they are still a small fraction among all web pages and 300 dimension embeddings may not have enough room to save those minor details. We demonstrated that even GloVe is not perfect on review-based tasks. What is even interesting is that the domain problem is word-by-word , not domain-by-domain. For some general stop words, we believe the representation from the natural distribution of web pages win as those words should be used for any domain and GloVe provides such generality that a domain corpus may not give. For domain specific-words, we don't wish distributions from irrelevant domains bias them. For example, we may only wish bright is closely associated with screen or keyboard but not sun in a Laptop domain. Thus, the ideal case is to combine them together and we show a simple way to do that. Task Representation Learning In a more general perspective, the domain is just one aspect of an end task. Since we focus on unsupervised learning to avoid heavy human efforts on annotation for a specific task, this may lead to representations more general to a wide spectrum of tasks but less specific to an end task. This is more important in contextualized representation learning as to whether certain contexts are important or not is very task-specific. We show that less supervised training data for an end task is unable to learn enough task representation and certain methods to bridge the gap is needed.","title":"Task Representation Learning"},{"location":"tdrl/#task-and-domain-representation-learning","text":"Our research focus is on unsupervised (or self-supervised) domain representation learning as strong supervision from humans is not AI enough. Even DL has the data-intensive generalization, it still (and may always) suffers from the domain problem in two perspectives: (1) the distribution of data is changed in an end task and certain types of data never appear before; (2) the majority wins the representation where domain-specific representations are squeezed but later challenged by a domain task. For each perspective, we focus on two-levels of domain representation, driven by two major breakthroughs in NLP research on deep learning(word embedding and contextualized representation learning).","title":"Task and Domain Representation Learning"},{"location":"tdrl/#out-of-distribution-domain-representation-learning","text":"Although most NLP tasks are defined on formal writings such as articles from Wikipedia, informal texts are largely ignored in many NLP tasks but are in huge amount. As such, representations learned from formal texts are challenged by domains with informal texts (which may differ in style-of-writing or rich opinions). Consequently, the representation is not general enough.","title":"Out-of-distribution Domain Representation Learning"},{"location":"tdrl/#word-level","text":"On the word level, we show that such change of domains makes word embeddings trained from Wikipedia not suitable for reviews.","title":"Word Level"},{"location":"tdrl/#context-level","text":"Similar things happen to as in contextualized representation learning. Via ablation study, we show that a language model ( BERT ) pre-trained on Wikipedia and Bookcorpus is incapable handle 3 review-based tasks well.","title":"Context Level"},{"location":"tdrl/#domain-representation-learning-for-minor-data","text":"Domain issues also come in even you have the domain data presented in your training corpus but are just not in the majority. This is caused by either the minor domain is challenged by the end task or fixed size of representation learning such that minor data must make rooms for major data. For example, on the word-level, some well-known word embeddings are trained from web pages in the world, where a certain amount of reviews may present. But they are still a small fraction among all web pages and 300 dimension embeddings may not have enough room to save those minor details. We demonstrated that even GloVe is not perfect on review-based tasks. What is even interesting is that the domain problem is word-by-word , not domain-by-domain. For some general stop words, we believe the representation from the natural distribution of web pages win as those words should be used for any domain and GloVe provides such generality that a domain corpus may not give. For domain specific-words, we don't wish distributions from irrelevant domains bias them. For example, we may only wish bright is closely associated with screen or keyboard but not sun in a Laptop domain. Thus, the ideal case is to combine them together and we show a simple way to do that.","title":"Domain Representation Learning for Minor Data"},{"location":"tdrl/#task-representation-learning","text":"In a more general perspective, the domain is just one aspect of an end task. Since we focus on unsupervised learning to avoid heavy human efforts on annotation for a specific task, this may lead to representations more general to a wide spectrum of tasks but less specific to an end task. This is more important in contextualized representation learning as to whether certain contexts are important or not is very task-specific. We show that less supervised training data for an end task is unable to learn enough task representation and certain methods to bridge the gap is needed.","title":"Task Representation Learning"}]}